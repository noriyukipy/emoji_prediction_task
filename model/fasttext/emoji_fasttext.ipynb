{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastTextでの学習\n",
    "\n",
    "fastTextで文の絵文字推定モデルを学習する。\n",
    "事前に `output` ディレクトリを作成すること。\n",
    "\n",
    "Notes:\n",
    "- fastTextでは内部で学習データをシャッフルしないので、事前にシャッフルすること https://github.com/facebookresearch/fastText/issues/74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Declare parameters set by `papermill` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "output_dir = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependent libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリのビルドとインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fastTest\n",
    "! cd $output_dir && wget https://github.com/facebookresearch/fastText/archive/v0.9.1.zip && unzip v0.9.1.zip && cd fastText-0.9.1 && make\n",
    "# Install python packages\n",
    "! pip install $output_dir/fastText-0.9.1/ janome sklearn seaborn matplotlib pandas pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib to draw graph\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test library\n",
    "\n",
    "Test your all the libraries used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset\n",
    "\n",
    "../../data/output/{train,test}.tsv を corpus ディレクトリにコピーする このディレクトリでコンテナを起動するとコピーできないため、jupyter notebookにはコマンドは記載していないが、記載するとすれば次のようになる\n",
    "\n",
    "$ cp ../../data/output/{train,valid,test}.tsv data/\n",
    "ディレクトリ構成\n",
    "\n",
    "```\n",
    ".\n",
    "├── README.md\n",
    "├── data/\n",
    "　   ├── train.txt\n",
    "      ├── valid.txt\n",
    "　   └── test.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストの前処理を行う関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "class PreprocessingTokenizer:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = Tokenizer()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self._tokenizer.tokenize(text, wakati=True)\n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "\n",
    "def format_data(tokenizer, in_fd, out_fd, random_state=0):\n",
    "    df = pandas.DataFrame({\"text\": [text for text in in_fd]})\n",
    "    df = df.sample(frac=1.0, random_state=random_state)\n",
    "    for i, line in enumerate(df[\"text\"].values):\n",
    "        try:\n",
    "            label, text = line.strip('\\n').split(\"\\t\")\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "        label_fasttext = \"__label__{}\".format(label)\n",
    "        text_fasttext = tokenizer.tokenize(text)\n",
    "        print(\"{} {}\".format(label_fasttext, text_fasttext), file=out_fd)\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(i+1, \"processed\")\n",
    "\n",
    "        \n",
    "def make_data(train_file, valid_file, test_file, train_out, valid_out, test_out):\n",
    "    tokenizer = PreprocessingTokenizer()\n",
    "\n",
    "    format_data(tokenizer, open(train_file), open(train_out, \"w\"))\n",
    "    format_data(tokenizer, open(valid_file), open(valid_out, \"w\"))\n",
    "    format_data(tokenizer, open(test_file),  open(test_out, \"w\"))\n",
    "\n",
    "    \n",
    "make_data(\"data/train.tsv\", \"data/valid.tsv\", \"data/test.tsv\", output_dir + \"/train.txt\", output_dir + \"/valid.txt\", output_dir + \"/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "\n",
    "def train(in_file, out_file):\n",
    "    model = fasttext.train_supervised(input=in_file)\n",
    "    model.save_model(out_file)\n",
    "    return model\n",
    "    \n",
    "print(\"training ...\")\n",
    "model = train(in_file=output_dir + \"/train.txt\", out_file=output_dir + \"/model.bin\")\n",
    "print(\"training ... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.test returns (number of samples, P@1, R@1)\n",
    "model.test(\"output/valid.txt\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid_dataset(fd):\n",
    "    res = []\n",
    "    for line in fd:\n",
    "        line = line.strip(\"\\n\")\n",
    "        tokens = line.split(\" \")\n",
    "        label = tokens[0]\n",
    "        text = \" \".join(tokens[1:])\n",
    "        res.append({\"label\": label, \"text\": text})\n",
    "    return res\n",
    "\n",
    "def predict(tokenizer, model, texts):\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        text_preprocessed = tokenizer.tokenize(text)\n",
    "        res = model.predict(text_preprocessed)\n",
    "        label = res[0][0]\n",
    "        score = res[1][0]\n",
    "        result.append({\"label\": label, \"score\": score})\n",
    "    return result\n",
    "\n",
    "\n",
    "class IdentityTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        return text\n",
    "\n",
    "val_dataset = load_valid_dataset(open(output_dir + \"/valid.txt\"))\n",
    "val_pred = predict(IdentityTokenizer() , model, texts=[item[\"text\"] for item in val_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score([x[\"label\"] for x in val_dataset], [x[\"label\"] for x in val_pred])\n",
    "print(\"accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate top-K accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(gold, pred, k):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(len(gold)):\n",
    "        assert len(pred[i]) == k\n",
    "        total += 1\n",
    "        if gold[i] in pred[i]:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "top_5_accuracy = top_k_accuracy(\n",
    "    [x[\"label\"] for x in val_dataset],\n",
    "    [model.predict(item[\"text\"], k=5)[0] for item in val_dataset],\n",
    "    k=5,\n",
    ")\n",
    "print(\"Top-5 accuracy:\", top_5_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "print(\"Accuracy\")\n",
    "pandas.DataFrame({\"Top 1\": accuracy, \"Top 5\": top_5_accuracy}, index=[\"fastText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrixを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "labels = list(sorted(set(x[\"label\"] for x in val_dataset)))\n",
    "conf_matrix = confusion_matrix([x[\"label\"] for x in val_dataset], [x[\"label\"] for x in val_pred], labels=labels)\n",
    "matplotlib.pyplot.figure(figsize = (20,20))\n",
    "seaborn.heatmap(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
