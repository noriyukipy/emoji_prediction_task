{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Flatten Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependent libraries\n",
    "This section installs required package. Version should be specified for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install janome==0.3.10 attrdict==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import attrdict\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters\n",
    "Declare parameters set by `papermill` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "name = \"test\"\n",
    "data_dir = \"data_sample\"\n",
    "output_dir =\"output\"\n",
    "num_words = 500\n",
    "embedding_size = 100\n",
    "batch_size = 32\n",
    "input_length = 50\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an attribute object `param` from parameters, then delete parameter variables to clean this namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_params = attrdict.AttrDict({\n",
    "    \"name\": name,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"num_words\": num_words,\n",
    "    \"embedding_size\": embedding_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"input_length\": input_length,\n",
    "    \"num_epochs\": num_epochs,\n",
    "})\n",
    "del data_dir\n",
    "del output_dir\n",
    "del embedding_size\n",
    "del num_words\n",
    "del batch_size\n",
    "del input_length\n",
    "del num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    wakati_tokenizer = JanomeTokenizer(wakati=True)\n",
    "    items = [x.split(\"\\t\") for x in open(path)]\n",
    "    return [wakati_tokenizer.tokenize(item[1]) for item in items], [item[0] for item in items]\n",
    "\n",
    "train_texts, train_labels = load_dataset(_params.data_dir + \"/train.tsv\")\n",
    "valid_texts, valid_labels = load_dataset(_params.data_dir + \"/valid.tsv\")\n",
    "test_texts, test_labels = load_dataset(_params.data_dir + \"/test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=_params.num_words)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "tokenizer.word_index\n",
    "\n",
    "# [TODO] process unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build labels\n",
    "# num_words is set to large number to cover all the labels\n",
    "label_tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, filters=\"\", lower=False)\n",
    "label_tokenizer.fit_on_texts(train_labels)\n",
    "label_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_words, embedding_size, label_size, input_length):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(_params.num_words, _params.embedding_size, input_length=input_length))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(label_size, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyper parameters fitting on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params):\n",
    "    label_size = len(label_tokenizer.word_index)+1  # The number starts from 0, so add one\n",
    "    model = build_model(params.num_words, params.embedding_size, label_size, params.input_length)\n",
    "    model.summary()\n",
    "    \n",
    "    # Prepare train/validation set\n",
    "    x_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=params.input_length)\n",
    "    y_train = np.array([label_tokenizer.word_index[l] for l in train_labels])\n",
    "    x_valid = pad_sequences(tokenizer.texts_to_sequences(valid_texts), maxlen=params.input_length)\n",
    "    y_valid = np.array([label_tokenizer.word_index[l] for l in valid_labels])\n",
    "\n",
    "    # Train\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\n",
    "    )\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=1, # Stop training if the monitor metric is not improved in 2 epochs in the row\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(params.output_dir, \"model.h5\"),\n",
    "            monitor=\"accuracy\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=params.output_dir,\n",
    "            histogram_freq=1,\n",
    "            embeddings_freq=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=_params.num_epochs,\n",
    "        batch_size=_params.batch_size,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "    )\n",
    "    return history.model\n",
    "\n",
    "_val_best_model = train(_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params, model):\n",
    "    # Prepare train/validation set\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=params.input_length)\n",
    "    y_test = np.array([label_tokenizer.word_index[l] for l in test_labels])\n",
    "    \n",
    "    model.summary()\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "    \n",
    "\n",
    "evaluate(_params, _val_best_model)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
