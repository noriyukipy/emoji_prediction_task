{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Flatten Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import dependent libraries\n",
    "This section installs required package. Version should be specified for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install janome==0.3.10 attrdict==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import attrdict\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters\n",
    "Declare parameters set by `papermill` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "_params = dict(\n",
    "    data_dir=\"data_sample\",\n",
    "    output_dir=\"output\",\n",
    "    num_words=500,\n",
    "    embedding_size=100,\n",
    "    batch_size=32,\n",
    "    input_length=50,\n",
    "    num_epochs=10,\n",
    "    seed=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert provided parameter dictionary to attribute object\n",
    "_params = attrdict.AttrDict(_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(_params.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Problem\n",
    "\n",
    "See README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataset\n",
    "\n",
    "See README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Evaluation Metrics\n",
    "\n",
    "See README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Determine Eavaluation Protocol\n",
    "\n",
    "See README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    wakati_tokenizer = JanomeTokenizer(wakati=True)\n",
    "    items = [x.split(\"\\t\") for x in open(path)]\n",
    "    return [wakati_tokenizer.tokenize(item[1]) for item in items], [item[0] for item in items]\n",
    "\n",
    "train_texts, train_labels = load_dataset(_params.data_dir + \"/train.tsv\")\n",
    "valid_texts, valid_labels = load_dataset(_params.data_dir + \"/valid.tsv\")\n",
    "test_texts, test_labels = load_dataset(_params.data_dir + \"/test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer\n",
    "_text_tokenizer = keras.preprocessing.text.Tokenizer(num_words=_params.num_words)\n",
    "_text_tokenizer.fit_on_texts(train_texts)\n",
    "_text_tokenizer.word_index\n",
    "\n",
    "# [TODO] process unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build label tmapper\n",
    "# num_words is set to large number to cover all the labels\n",
    "_label_tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, filters=\"\", lower=False)\n",
    "_label_tokenizer.fit_on_texts(train_labels)\n",
    "_label_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def save(tokenizer, name, output_dir):\n",
    "    config = tokenizer.to_json()\n",
    "    with open(os.path.join(output_dir, name), \"w\") as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "\n",
    "save(_text_tokenizer, \"text_tokenizer.json\", _params.output_dir)\n",
    "save(_label_tokenizer, \"label_tokenizer.json\", _params.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data to convert data to input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(texts, labels, text_tokenizer, label_tokenizer, max_len):\n",
    "    x_data = pad_sequences(text_tokenizer.texts_to_sequences(texts), maxlen=max_len)\n",
    "    y_data = np.array([label_tokenizer.word_index[l] for l in labels])\n",
    "    return x_data, y_data\n",
    "\n",
    "# Prepare train/validation set\n",
    "_x_train, _y_train = build_data(train_texts, train_labels, _text_tokenizer, _label_tokenizer, _params.input_length)\n",
    "_x_valid, _y_valid = build_data(train_texts, train_labels, _text_tokenizer, _label_tokenizer, _params.input_length)\n",
    "_x_test, _y_test = build_data(train_texts, train_labels, _text_tokenizer, _label_tokenizer, _params.input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-8. Develop and Tune Models\n",
    "\n",
    "6. Develop a model to overcome baseline model\n",
    "7. Develop a overfitting model\n",
    "8. Regularize the model and tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_words, embedding_size, label_size, input_length):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(_params.num_words, _params.embedding_size, input_length=input_length))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(label_size, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, label_size, x_train, y_train, x_valid, y_valid):\n",
    "    model = build_model(params.num_words, params.embedding_size, label_size, params.input_length)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\n",
    "    )\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=1, # Stop training if the monitor metric is not improved in 2 epochs in the row\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(params.output_dir, \"model.h5\"),\n",
    "            monitor=\"accuracy\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=params.output_dir,\n",
    "            histogram_freq=1,\n",
    "            embeddings_freq=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=_params.num_epochs,\n",
    "        batch_size=_params.batch_size,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "    )\n",
    "    return history.model\n",
    "\n",
    "# label starts from 0 index, so 1 is added\n",
    "_val_best_model = train(_params, len(_label_tokenizer.word_index)+1, _x_train, _y_train, _x_valid, _y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params, model, x_test, y_test):\n",
    "    model.summary()\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "    \n",
    "\n",
    "evaluate(_params, _val_best_model, _x_test, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(params, x_test, y_test):\n",
    "    model = keras.models.load_model(os.path.join(params.output_dir, \"model.h5\"))\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "    \n",
    "load_and_evaluate(_params, _x_test, _y_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
